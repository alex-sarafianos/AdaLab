
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{amssymb}

\begin{document}

\newcommand{\probmodelspace}{\mathcal{P}_\mathcal{M}}
\newcommand{\modelspace}{\mathcal{M}}
\newcommand{\expspace}{\mathcal{E}}

\author{Jan Ramon}
\title{An ML sketch based on the 22/1/2016 discussion}
\date{24/1/2016}
\maketitle

\section{Model and experiment space declarations}

Declarations should be constructed for the \textit{model space} and \textit{experiment space}.  These specifications may be refined during the project, it is essential to have a repository which always has the most recent specifications supported by all modules (and possibly experimental versions not yet supported by all modules).

This document will represent specifications in logic (prolog facts),
there are some specific formalisms around, e.g., for gene regulatory networks,
but they are not always compatible with each other nor sufficiently general to
incorporate everything we want to model.
Nevertheless, during implementation more suitable representations could be developed.

\subsection{Experiment space}

An environment state is a data structure representing the state of the environment at a certain time point.  This includes amongst others the concentration of compounds (sugar, alcohol, ...) and temperature (see technical documents).  

The specification of an experiment is a tuple $expSpec(InitEnv,IncEnv,Strain,Output)$ where
\begin{enumerate}
\item $InitEnv$ is the initial environment state
\item $IncEnv$ is a function which gives for each time point $t$ the external changes to the environment, such as addition of compounds (sugar, alcohol, ...) and increase/decrease of temperature.
\item $Strain$ is the strain used (e.g., knocked out genes)
\item $Output$ is a list of measurements made during the experiment, e.g., a point on the growth curve every 20 minutes, the amount of sugar at the end.
\end{enumerate}

The experiment space declaration is a set of statements defining this data structure in more detail, e.g.

\begin{verbatim}
experiment_specification(exampleExpSpec).
expSpec_initEnv(exampleExpSpec, Feature, ValueRange) :-
   envStateSpec(exampleEnvStateSpec, Feature, ValueRange).
expSpec_incEnv(exampleExpSpec, addComp(Compound, Amount)) :- 
   member(Compound, [sugar,alcohol]), 
   member(Amount,[1,2,3,4]). % in vol%
expSpec_incEnv(exampleExpSpec, deltaTemp(DT)) :-
   member(DT,[-1,1]). % in degrees Celsius
expSpec_envState(exampleExpSpec, exampleEnvStateSpec).
expSpec_output(exampleExpSpec, Time, yeastGrowth(Time)) :- 
   Time = 20 
   ; expSpec_output(exampleExpSpec, PrevTime, yeastGrowth(PrevTime)),
     Time is PrevTime + 20. % in minutes
expSpec_strain(exampleExpSpec, exampleStrainSpec).

envStateSpec(exampleEnvStateSpec, temp, real(0,40)).
envStateSpec(exampleEnvSTateSpec, sugarConc, real(0,1)).

strainSpec(exampleStrainSpec, knockOut(Gene), bool) :-
   member(Gene, [gene1, gene2]).
strainSpec(exampleStrainSpec, greyScale(Gene), real(0,1,Resolution)) :-
   member(Gene, [gene3, gene4]), Resolution=0.1.
\end{verbatim}

{\small{For those not familiar with prolog, it is easy from the above the ``ground'' these specifications, i.e. writing them as instantiated facts.  Those liking lifted inference can keep the implicit clauses.}}

We will denote experiment spaces with $\expspace$.

\subsection{Model space}

The state of an experiment is a triple $S=state(Strain, YeastState, EnvState)$ (1) strain information, (2) the state of the yeast (gene expression levels, metabolite concentrations, ...) and (3) the environment (temperature, concentration of compounds, ...)

The model describes how the state of an experiment evolves over time.
An experiment $Exp$ starts at time $0$.
If the specification of the experiment $Exp$ is $expSpec(InitEnv,IncEnv,Strain,Output)$, then $state(Exp,0)$

The model space declaration defines the parameters of the model.  For example:

\begin{verbatim}
modelSpec(exampleModelSpec).
modelSpec_strain(exampleModelSpec, exampleStrainSpec). 
modelSpec_parameter(exampleModelSpec, geneRegulateEdge(G1,G2), real(-1,1,0.1)) :-
   % +1 is upregulation, -1 is inhibition
   exampleModelGene(G1), exampleModelGene(G2).
modelSpec_parameter(exampleModelSpec, geneTempRegulate(G1), real(-1,1,0.1)) :-
   % for genes influenced by the temperature:
   % strength of regulation is the (temperature minus 20Celsius)
   % times the value of this parameter.
   exampleModelGene(G1).

exampleModelGene(gene1).
exampleModelGene(gene2).
exampleModelGene(gene3).
exampleModelGene(gene4).
\end{verbatim}

We will denote model spaces with $\modelspace$.

\subsubsection{Including both dependencies and functions parameters}
The above example only defines network structure parameters, describing the force of dependencies between genes. However, this assumes that the expression level function of a gene given its parents (regulators, temperature, metabolites, ...) is not part of the model parameters. We propose here a more general adaptation of previous example for less restricted expression functions of genes given their parents:

\begin{verbatim}
modelSpec(exampleModelSpec).
modelSpec_strain(exampleModelSpec, exampleStrainSpec). 
modelSpec_parameter(exampleModelSpec, geneRegulateEdge(G1,G2), bool) :-
   exampleModelGene(G1), exampleModelGene(G2).
modelSpec_parameter(exampleModelSpec, geneTempRegulate(G1), bool) :-
   % for genes influenced by the temperature:
   % strength of regulation is the (temperature minus 20Celsius)
   % times the value of this parameter.
   exampleModelGene(G1).
modelSpec_parameter(exampleModelSpec, geneExpressionFunction(G1), Function) :-
   %Function is of the form functionName(Gene, ParentsList, GeneExpressionLevel) 
   %where the parents are the nodes affecting its expression level
   %   (transcription factors, temperature, ...)
   exampleModelGene(G1), functor(Function, _, 3).
%An example of function in an additive context could be:
functionExample(Gene, [R1,R2,R3], Level) :- expressionLevels([R1,R2,R3], [E1,E2,E3]), 
   Level is -0.6 * E1 + 0.5 * E2 + 1 * E3.
exampleModelGene(gene1).
exampleModelGene(gene2).
exampleModelGene(gene3).
exampleModelGene(gene4).
\end{verbatim}

Note that here, a Function is an arity 3 predicate which is specific to a gene (with full instanciations of all parameters). The Function space here is not constrained in order to match many machine learning strategies. However, we could refine the constraints on the Function parameter for a given strategy by allowing its name to be in some subset of possible functors. For example, we could only allow Function values being composition of $noisy_or$ and $noisy_and$ predicates.

\subsubsection{Dynamic Model through K slices of time}

The model under study is dynamic, allowing to predict the evolution of the biological system for different time periods. An obvious restriction in the model is that a time step state does no depend on the future steps. However, dependencies should not necessarily assume that t step depends on t-1 step only (2 slices of time model). It may indeed rely on more previous steps, but also allow for intra-time step dependencies (describing observed correlations which could allow to guide the structure learning process). Thus, defining dependencies between the different concepts of the model should include time steps values to make explicit the actual dependency which is occuring. An example of such addition would be:
  
\begin{verbatim}
modelSpec(exampleModelSpec).
modelSpec_strain(exampleModelSpec, exampleStrainSpec). 
%TimeOffset = i means we have a dependency from t minus i value of G1 to t value of G2.
%TimeOffset = 0 describes an intra-time step correlation
modelSpec_parameter(exampleModelSpec, geneRegulateEdge(G1,TimeOffset,G2), bool) :-
   exampleModelGene(G1), exampleModelGene(G2), member(TimeOffset, allowedTimeRange).
modelSpec_parameter(exampleModelSpec, geneTempRegulate(G1,TimeOffset), bool) :-
   exampleModelGene(G1), member(TimeOffset, allowedTimeRange).
modelSpec_parameter(exampleModelSpec, geneExpressionFunction(G1), Function) :-
   %Function is of the form functionName(Gene, ParentsList, GeneExpressionLevel) 
   %where the parents are (Regulator, TimeOffset) pairs
   exampleModelGene(G1), functor(Function, _, 3).
%An example of function in an additive and dynamic context could be:
%Remember that each Ri is indeed a (TF, TimeOffset) pair!
functionExample(Gene, [R1,R2,R3], Level) :- expressionLevels([R1,R2,R3], [E1,E2,E3]), 
   Level is -0.6 * E1 + 0.5 * E2 + 1 * E3.
exampleModelGene(gene1).
exampleModelGene(gene2).
exampleModelGene(gene3).
exampleModelGene(gene4).
\end{verbatim}

\subsubsection{Working with uncertainty}

There are several reasons why we should consider uncertainty in our biological model. Some main causes include: 1) the lack of knowledge (we can observe not deterministic dependencies between genes because we miss some co-regulators for example, but our partial knowledge already allows for better prediction than not considering it, so we should keep it); 2) the noise in observations (we have different behaviour for apparent same experimental conditions which can in fact be different for some feature not described in the dataset); 3) the system uncertainty itself (even if some coumpounds of a chemical reaction are present, molecules may not collide and provoke the reaction, or not enough for a regulation to occur); 4) the system state uncertainty (we do not know some of the gene states and thus we have to marginalize over their values to compute our beliefs on other genes states). 

Due to these sources of uncertainty, the model should be able to work not only with gene expression functions but also distributions. A simple example of updated program in this context could be:
\begin{verbatim}
modelSpec_parameter(exampleModelSpec, geneDistribution(G1), Distrib) :-
   %Distribution is of the form 
   %   distribName(Gene, ParentsList, GeneExpressionLevels, Probas) 
   %where the parents are (Regulator, TimeOffset) pairs
   exampleModelGene(G1), functor(Function, _, 4).
%An example of function in an additive and dynamic context could be:
%Remember that each Ri is indeed a (TF, TimeOffset) pair!
%For readability, we consider more discretized gene states
functionExample(Gene, [R1,R2,R3], Levels, Probas) :- 
   expressionLevels([R1,R2,R3], [-1,-1,0]), Levels = [-1,0,1], 
                                            Probas = [0.7,0.2,0.1].
functionExample(Gene, [R1,R2,R3], Levels, Probas) :- 
   expressionLevels([R1,R2,R3], [-1,-1,1]), Levels = [-1,0,1], 
                                            Probas = [0.5,0.3,0.2].
%[... and so on for other values of [R1,R2,R3]...]
\end{verbatim}

Note that such distributions are not necessarily conditional probability table requiring one probability value (parameter) per gene x parent configuration. They can also be other usual distributions requiring less parameters.

\subsubsection{Domain theory embedding}

The model space should be able to include domain theory. In the following, we give several examples of such knowledge.

\paragraph{Transcription Factors and Target Genes}

Not all genes are able to regulate siblings. Assigning to each gene in the model its type (a.k.a. its biological function) could be useful to limit the number of parameters to learn or structures to browse. An update to previous definition in this direction could be:

\begin{verbatim}
modelSpec_parameter(exampleModelSpec, geneRegulateEdge(G1,TimeOffset,G2), bool) :-
   exampleTranscriptionFactor(G1), exampleModelGene(G2), 
   member(TimeOffset, allowedTimeRange).
exampleTranscriptionFactor(G1) :- exampleModelGene(G1).
exampleTranscriptionFactor(gene1).
exampleTranscriptionFactor(gene3).
exampleModelGene(gene2).
exampleModelGene(gene4).
\end{verbatim}

\paragraph{Activators and inhibitors aggregation}

As stated in the above examples, regulation between genes can be either an activation or an inhibition. However, the individual expression of each regulator is not necessarily what matters the most in the end when trying to simulate the target gene's expression level.
Indeed, some aggregation of activators and inhibitors could be more adequate information to infer the gene's expression level. Thus, including the activator and inhibitor roles for regulation dependencies could allow to define aggregation-oriented regulation rules, e.g.:

\begin{verbatim} 
%We must also add a constraint stating that a TF 
%cannot both activate and inhibit a same gene.
modelSpec_parameter(exampleModelSpec, geneActivateEdge(G1,TimeOffset,G2), bool) :-
   exampleTranscriptionFactor(G1), exampleModelGene(G2),
   member(TimeOffset, allowedTimeRange).
modelSpec_parameter(exampleModelSpec, geneInhibitEdge(G1,TimeOffset,G2), bool) :-
   exampleTranscriptionFactor(G1), exampleModelGene(G2),
   member(TimeOffset, allowedTimeRange).
modelSpec_parameter(exampleModelSpec, geneTempRegulate(G1,TimeOffset), bool) :-
   exampleModelGene(G1), member(TimeOffset, allowedTimeRange).
modelSpec_parameter(exampleModelSpec, geneExpressionFunction(G1), Function) :- 
   exampleModelGene(G1), functor(Function, _, 3).
%An example of function with the activation and inhibition roles could be:
functionExample2(Gene, Parents, Level) :- activators(Gene, Parents, Activators),
   inhibitors(Gene, Parents, Inhibitors), expressionLevels(Activators, LEA), 
   expressionLevels(Inhibitors, LEI), mean(LEA, MEA), mean(LEI, MEI),
   Level is 0.6 * (MEA - 2 * MEI).
exampleTranscriptionFactor(G1) :- exampleModelGene(G1).
exampleTranscriptionFactor(gene1).
exampleTranscriptionFactor(gene2).
exampleTranscriptionFactor(gene3).
exampleModelGene(gene4).
\end{verbatim}

\paragraph{Co-regulations}

In addition to activators and inhibitors being potentially more important as an aggregation than as separate contributors to a target expression level, co-regulation of several genes seem to be an important knowledge to take into consideration beyond pairwise regulations. Thus, the model should be able to include lists of co-regulators which could be jointly added as parent of target genes in order to avoid missing them when subsets of these co-regulators would not be detected as regulators at all by some machine learning heuristics (e.g. by the use of iterative forward feature selection mechanisms). An extension to the previously defined PROLOG program could be the following:

\begin{verbatim} 
%We must also add a constraint stating that a TF 
%cannot both activate and inhibit a same gene.
modelSpec_parameter(exampleModelSpec, activateEdge(P,TimeOffset,G), bool) :-
   exampleTranscriptionFactor(P), exampleModelGene(G), 
   member(TimeOffset, allowedTimeRange) ;
   exampleCoRegulator(P), exampleModelGene(G),
   member(TimeOffset, allowedTimeRange).
modelSpec_parameter(exampleModelSpec, inhibitEdge(P,TimeOffset,G), bool) :-
   exampleTranscriptionFactor(P), exampleModelGene(G),
   member(TimeOffset, allowedTimeRange) ;
   exampleCoRegulator(P), exampleModelGene(G),
   member(TimeOffset, allowedTimeRange).
modelSpec_parameter(exampleModelSpec, geneTempRegulate(G1,TimeOffset), bool) :-
   exampleModelGene(G1),member(TimeOffset,allowedTimeRange).
%We have to assign expressionLevels to
%co-regulators the same way as for genes, using functions
%We thus generalize the previous geneExpressionFunction parameter to nodeFunction
modelSpec_parameter(exampleModelSpec, nodeFunction(N), Function) :- 
   exampleModelGene(N), functor(Function, _, 3) ;
   exampleCoRegulator(N), functor(Function, _, 3).
%The example function does not change since functions are also defined for
%co-regulators and thus allow to compute their expresion levels
functionExample2(Gene, Parents, Level) :- activators(Gene, Parents, Activators),
   inhibitors(Gene, Parents, Inhibitors), expressionLevels(Activators, LEA), 
   expressionLevels(Inhibitors, LEI), mean(LEA, MEA), mean(LEI, MEI),
   Level is 0.6 * (MEA - 2 * MEI).
exampleTranscriptionFactor(G1) :- exampleModelGene(G1).
exampleTranscriptionFactor(gene1).
exampleTranscriptionFactor(gene2).
exampleTranscriptionFactor(gene3).
exampleModelGene(gene4).
%CoRegulator definition composed of [gene1, gene3]
exampleCoRegulator(cr1).
exampleCoRegulatorGene(cr1, gene1).
exampleCoRegulatorGene(cr1, gene3).
\end{verbatim}

We will denote model spaces with $\modelspace$.

\subsection{Cost and objective functions}

We will have declarations of the cost of experiments, e.g.,

\begin{verbatim}
expCost(exampleExpSpec,newknockoutStrain, 100).
expCost(exampleExpSpec,newGreyScaleStrain, 1000).
expCost(exampleExpSpec,experiment, 1).
expCost(exampleExpSpec,initEnv(Feature, Value), 0) :-
   expSpec_initEnv(exampleExpSpec, Feature, ValueRange),
   acceptable_value(ValueRange, Value). 
   % let's assume that all value ranges are finite due to 
   % (possibly fine) discretization.
expCost(exampleExpSpec,incEnv(addComp(Compound,Amount), 0.5).
\end{verbatim}

and we need a loss function to compare two experiment outputs (a predicted and actual one).
The loss function can use for instance extracted parameters related to diauxic shift from both curves such as: 
\begin{itemize}
 \item Lag time: time it takes for a sign of growth in the curve
 \item Linear slope of the exponential phase
 \item Linear slope of the post-diauxic phase
 \item Exponential phase duration
 \item Post-diauxic phase duration
 \item Diauxic shift duration
 \item Diauxic shift (binary value)
\end{itemize}

Alternatively the loss function could also be calculated directly using time series of growth curves by using for instance 
dynamic time warping techniques. The loss function can also include a regularization term e.g. to favour simpler models.

About global loss function, in order to evaluate an overall performance of our model, if we assume that we have a few points per time series and that the time alignment between predicted and real curves is good (if not perfect), maybe we could consider the Euclidean distance in the curves space. If the number of points is huge ($>n$ for a $n$ to determine ...), we could maybe try to simplify the curve before computing the distance (Fourier transform?). In a probability framework setting, we could also define a model which would allow to compute some conditional (log) likelihood of the form $(log) P(curve_{t1}, ..., curve_{tend}|Model)$.

It could also be important to create more local scores for the different parts of the growth curve, in order to guide the refinement process. Indeed, if the typical yeast growth curve during diauxic shift contains 5 time periods with typical behaviour, and if the error of some prediction is local to some specific period, then maybe we could restrict the set of genes to update.

\subsection{Experimental observation database}

Initial experimental conditions is a data structure specifying the initial values of all variables declared in the experiment initialisation space, which is a part of the experiment specification space described above.

An experiment update strategy is a data structure specifying as a function of time the changes to the environment performed during the experiment.  This data structure should contain updates in the space of experiment updates, which is a part of the experiment space described above.

An experiment input is a tuple $expInput(ExpInit,ExpInc)$
where $ExpInit$ is an initial experimental condition
and $ExpInc$ is an experiment environment update strategy.

An experiment output is an assignment of values to all measurements declared in the experiment output space, which is a part of the experiment specification space described above.

Once experiments are performed, a database can be constructed consisting of
pairs $(ExpInput,ExpOutput)$.

\subsection{Model probability distribution spaces}
\label{sec:modProbDist}
Given a declaration of a model space $\modelspace$ as described above, one can also define a probability space $\probmodelspace$ over $\modelspace$.
Even though inference algorithms can use internally any apropriate
probability distributions, one can restrict priors over model space
to independent priors on all the parameters, e.g. a prior could be specified as

\begin{verbatim}
model_prior(exampleModelPrior, geneRegulateEdge(G1,G2), [0.1:0,0.9:1]) :- 
   zimmerman(edge(G1,G2)).
model_prior(exampleModelPrior, geneRegulateEdge(G1,G2), [0.95:0,0.05:1]) :- 
   not(zimmerman(edge(G1,G2))).
model_prior(exampleModelPrior, geneTempRegulate(G1), [0.05:-1,0.05:-0.5,0.8:0,0.05:0.5,0.05:1]).
\end{verbatim}

A disadvantage of this strategy is that it may insufficiently take into account
\begin{itemize}
\item the prior that simpler models are more likely.
\item local patterns of which one could estimate the frequency by pattern mining (e.g. cyclicly directed triangles are unlikely).
\end{itemize}

\section{Adalab system components}

\subsection{Simulator}
The simulator takes as input:
\begin{enumerate}
\item a model space declaration
\item a compatible experiment space declaration
\item experiment initial conditions (consistent with the environment initialization space and strain space defined)
\item experiment environment updates (consistent with the environment increment space defined).
\end{enumerate}

and returns a set of values with the structure defined by the experiment output space declaration.


An additional argument can be provided to the simulator, specifying uncertainty on the model (see e.g. Section \ref{sec:modProbDist}).  In that case, the simulator will simulate several times, sampling from the model probability distribution.  
This could for instance include probabilities of certain edges or (continuous) probability density functions of parameters of the model. Alternatively some inherent noise to the gene regulation can be modelled by adding some noise (according to a prespecified
noise function) to the levels of gene expression(continuous) or by adding a probability of flipping a bit in the boolean case. The latter is currently already implemented in the simulator.

\subsection{Model parameter estimation}

A model parameter estimation algorithm is an algorithm taking as input
\begin{enumerate}
\item a declaration of a model space $\modelspace$.
\item a declaration of a compatible experiment space $\expspace$
\item a model space prior $P_{prior}$
\item a database of experimental observations $D$.
\end{enumerate}

and outputs a set of model parameters which best explains the observations under the model space prior, e.g.
$\hbox{argmax}_{m\in\modelspace} P_{prior}(m)P(D|m)$

Many different strategies are plausible, a naive one would just perform a hill climbing (in continuous or discrete space).
The latest version of the continuous simulator influence is modelled as a linear combination of a gene's regulators by means of the following equation:
\begin{equation}
 g_i(t+1) = \sum_{j : g_j \in Pa(g_i) \cup \{g_i\} } w_{ij}g_j(t) + \beta_{i}
 \label{eq:cont_sim}
\end{equation}
where $g_k$ are genes, $t$ is time, $Pa(\dot)$ indicates the parents of a gene, $w_{ij}$ are weights and $\beta_i$ is an intercept. 
Given this equation, we can define a matrix of parameters related to the adjacency matrix of the underlying network:
\begin{equation}
\mathbf{W} = \begin{bmatrix} 
w_{1,1} &\cdots & w_{1,n}\\
\vdots & \ddots & \vdots \\
w_{n,1} & \cdots & w_{n,n} 
      \end{bmatrix}
\end{equation}


and

\begin{equation}
 \mathbf{\beta} = \begin{bmatrix}
      \beta_1 \\
      \vdots \\
      \beta_n 
     \end{bmatrix}
\end{equation}

With $w_{i,j}$ is non-zero if there is a directed edge from node $i$ to $j$ and the value being the influence that node $i$ has on $j$ and $\beta_i$ is a constant term indicating the behaviour on the level of a node $i$ 
when there is no interaction (i.e. the incoming nodes have level 0) cfr. equation \ref{eq:cont_sim}. Which gives us the following set of parameters:
\begin{equation}
 \mathbf{\theta} = \begin{bmatrix}
                     w_{1,1} \\
                     \vdots \\
                     w_{n,n} \\
                     \beta_1 \\
                     \vdots \\
                     \beta_n
                    \end{bmatrix}
% 
\end{equation}
The parameters of this model have been estimated with time series datasets of gene expression data. This (simplified) model could already be used in a first phase, before going to more elaborate models.

\subsection{Experiment selection}

An experiment selection algorithm takes as input


A model parameter estimation algorithm is an algorithm taking as input
\begin{enumerate}
\item a declaration of a model space $\modelspace$.
\item a declaration of a compatible experiment space $\expspace$
\item a model space prior $P_{prior}$
\item a database of earlier experimental observations $D$.
\item an experiment cost function definition
\item a loss function
\end{enumerate}

and outputs a list of experiments which will be most informative / cost-efficient.

One (common) strategy could be to sample from the experiment space, and select the experiment which is most cost-effective and most different from earlier selected experiments.

Ways to determine the informativeness of experiments could include e.g.:
\begin{itemize}
\item the variation of experiment results around the selected experiment (e.G. by simulating the experiment with a bit of noise on the model).
\item an experiment between two experiments with different outcomes.
\item Expected model change \cite{cai2013maximizing}, i.e. choose the experiment for which the model (parameters) will change the most
\item Using a set of different models (e.g. obtained via a parameter estimation algorithm with a stochastic element), select the most uncertain (classification) or with highest variance (regression) experiment, cfr. QBC
\end{itemize}
Enforcing diverse experiments has been done in the past \cite{xu2007incorporating} by adding a diversity term to the ``general'' informativeness measure. Examples here could be using a distance metric such as cosine similarity, euclidian distance, ...
 
To estimate usefulness of an experiment, one needs an efficient way to 
move through the experimental space. As the experimental space is huge, an exhaustive search of experiments will not be feasible. A first approach (equivalent to pool-based sampling) could be:

\setlength{\textfloatsep}{10pt}% Remove \textfloatsep

\begin{algorithmic}
\Function{experiment\_sampling}{$\expspace$, batchSize, $D$, sampleSize}
\Require Experimental space $\expspace$, batchSize, database with previous experiments $D$, sampleSize
\State batch $\leftarrow \emptyset$
\State experimentSamples $\leftarrow$ sampleExperiments($\expspace$, $D$, sampleSize)
   \While{$|$batch$| < $ batchSize }
   \For{ experimentSample $\in$ experimentSamples }
     \State double Informativeness $\leftarrow$ determineInformativeness(experimentSample, $\expspace$, $D$, batch)
   \EndFor
   \State batch $\leftarrow$ batch $\cup$ \{experiment with highest informativeness\}
   \State experimentSamples $\leftarrow$ experimentSamples $\setminus$ \{experiment with highest informativeness\}
   \EndWhile \\
   \Return batch
\EndFunction
\end{algorithmic}

Alternatively, we can also create an experiment de novo by using existing optimization techniques. Let $\phi$ denote some informativeness function, $\mathbf{x}$ the experimental factors, with $N$ the amount of experimental factors 
and let $c_{i,min}$ and $c_{i,max}$ denote respectively the lower and upper bound for experimental factor $i$, we can define the optimization problem as follows:
\begin{equation}\label{eq:opt_data_select}
 \begin{aligned}
  & max_{\mathbf{x}} & & \phi(\mathbf{x}) \\
  & \text{s.t.} & & \forall i \in N: c_{i,min} \leq x_i \leq c_{i,max} \\
 \end{aligned}
\end{equation}
Additional constraints can be added depending on existing knowledge of the experimental space.
Depending on the mathematical properties of $\phi$, we can then employ different appropriate optimization techniques (constrained nonlinear optimization).
In pseudocode:
\begin{algorithmic}
  \Function{optimal\_selection}{$\expspace$, batchSize, $D$}
  \Require Experimental space $\expspace$, batchSize, database with previous experiments $D$
  \State  batch $\leftarrow \emptyset$ 
  \While{ $|$batch$| < $ batchSize }
  \State newExperiment $\leftarrow$ optimalExperiment($\expspace$ | batch, $D$ )\Comment{cfr. equation \ref{eq:opt_data_select} }
  \State batch $\leftarrow$ batch $\cup$ \{newExperiment\}
  \EndWhile \\
  \Return batch
  \EndFunction
\end{algorithmic}
\bibliographystyle{plain}
\bibliography{mlrefs}
\end{document}
